+++
date = "2018-04-08T17:36:00-04:00"
draft = false
visible = false
title = "<![CDATA[<br>I, bobbit, am trying to take over the world, and there's nothing you can do to stop me.]]>"
description = "waht does this do"
tags = [ "Ethics" ]
categories = [ "Ethics" ]
series = [ "Ethics" ]
+++

AI: Artificial Inteligence is just one of the worst Spielberg movies, but then, did he ever really have any good ones?

Wait, not that AI? Well, a program is generally considered an AI if it solves problems using something at all considered intelligent, at least that's what
the first article said. It then goes on to divide AI into "strong" and "weak", where "strong" is meant to resemble human intelligence and "weak" is meant
to do whatever as long as it works. It also divides AI into "general", which can reason in general, and "narrow", which is tailored to a specific problem. Obviously
a real strong AI would be quite similar to human intelligence, since that's literally what that means. But I very much agree with Searle in that strong AI is impossible,
though I think that might mean human intelligence is, too, which I'd believe. I've been on the internet before, there's no intelligence there. All of the weak AI
hardly resemble human intelligence to me, but I'm not a brainiologist, so I'm not qualified to speak on that.

Weak Artificial Intelligence is certainly viable. I'm not quite sure AlphaGo and Watson are good arguments for that, though, since they literally play
relatively simply games. They do show that we can make computer programs that are much better than people at certain tasks, but again, I think we all already
knew that - a program to find the product of the logs of all numbers between 1 and 3000 is very simple to write, and will do it much faster than any human.
I agree with the `Is AlphaGo Really Such a Big Deal?` article - AlphaGo was not some huge breakthrough, but was instead as an example of the development of 
systems that can learn to recognize patterns. I've heard from HackerNews that Watson is basically just IBM's analytics division and they hardly actually use
actual AI as much as they'd like everyone to believe, so I'm inclined to call Watson a gimmick. I guess my main issue with these things is they are things that
humans are naturally bad at. We have very limited memory compared to machines, so of course a machine should be able to beat us at these tasks. I find that recent
developments in image processing, fueled heavily by machine learning, that have lead to the start of self driving cars are much more promising. Driving is a task
that humans are relatively well suited, but we get distracted, or fail to notice something, or the roads are wet, or whatever, so maybe computers can learn to be
better than us. 

The Turing Test is and has always been a load of garbage. In my Second Philo we covered the Turing Test, a number of potential substitutes, Searle's argument, and a 
few other topics. Basically, none of the alternatives to the Turing Test work well, and this whole thing is a big philosophical problem, not just for machines, but
for everything else. It is easy for me, an individual who can think and whatnot, to believe that _I_ am intelligent, but it is very hard for me to
know that _anyone else_ is intelligent. The Turing test, as I see it, has two big flaws. It only tests for a simulation of intelligence, that is, Turing says
if it looks intelligent, then it is, but that's wrong. It also only tests for intelligence that is similar to our own. I'm sure there's some Star Trek episode
where some species is super intelligent but unable to speak (The Devil in the Dark, and a few others that I can't remember). 

Anyways, you could conceiveably write a giant switch statement for every response to any question under, say, 400 characters, and have
it remember the past 7 or so questions (roughly what a human can keep in working memory), and I would bet that it could fool anyone. But it is very obviously
not intelligent. John Searle says that a computer is basically always just a guy with a book in a room, reading off inputs and then producing output. For the most
part, he is totally right. In his anology the processor is the man and the program is the book. Self modifying programs do exist, but at any given time, the processor
is just following the book. Technically, we can model the entire computer, all of its bits in memory, including registers, as a simple but incredibly large deterministic
finite automaton - for any given state in RAM there is exactly one next state, and self modifying programs do not modify this automaton, they merely change
the state just like any other program. The only thing that might break the chinese room is the idea of experience - a computer on its own has finite memory, but
technically it can have memory added (more disk space, etc.), where it could perhaps store what it sees, and then modify itself. At any given moment it is still just
a big DFA, but maybe if it changes enough it can be something more. Maybe. I'm pretty firmly in the "computers can only be dumb automatons" camp.

Computers might be able to have minds though. The problem a lot of people have with Searle's argument is that it might mean people aren't intelligent either. But that
could be true, and maybe it is possible for computers to have minds like humans. Conceivably, you could build a neural network that pretty exactly models the human
brain, maybe. Searle thinks biology is somehow special. I guess I am a dualist and think maybe souls or some nonsense, but whatever. It at least seems possible to
simulate a rat brain in a computer, at least, maybe some day. The article that tells me to not be a chesudfavist about thinking, well, maybe thought it impossible.
Anyways, if a computer can simulate a rat brain, to its entirety, I would think that maybe it deserves the same ethics and morality that we give animals. Well,
if it is just a simulation, we run into my issue above, where a simulation really isn't the same thing. This is hard. Well, I still think we ought to err on the
safe side, and treat virtual minds with respect. If we are just biological computers, then there is no thought, and ethics and all that are irrelevent, and we should
just all become nihilists. Or we can extend the same rights to all of the simulated minds, and live in constant existential dread over the fact that we are mere automata.

Speaking of dread, AI is dangerous. I think it is absurd that some people fear AI taking over the world, or just instantly becoming super intelligent by
constantly making new intelligence. Like, what hardware is this super AI running on? We're running hard into a single core speed wall, so the only way
to get any faster is multicore, and very interestingly, if the AI uses the internet or something to spread out its mind to enough computers to be smart, it will
have huge latency, and will be dumber for it. The AI would have to be within one machine, or at least one very fast network, to be able to do anything, and I just
don't see us getting there in my lifetime. (Part of why I love computer science is it tells us what we can do with math, what fundamental limits reality has
on computing things. I also believe that there are not only fundamental constraints on mathematics, but on intelligence itself.) I am not afraid of this super AI
that rules the world, or any machine uprising. It seems the easy part is preventing machines from doing bad things - we already have humans verify computer output today.
What I am afraid of is that AI will upset the way the economy works so much that the only people who earn money
are the ones who already have money and own the companies that the AI works for. We could totally see a useless class. I've argued in the past that the only job
AI won't take away is the jobs the art majors somehow got. Not because AI can't make things that resemble high quality art, but because art is not the kind of thing
a computer can make - art is human expression, and I figure it will always have value to other humans. So, what I'm afraid of, is that the english major will have
the money and the software programmer will be replaced by Microsoft Visual Studio 2050. 
Though, [I can't wait for the future of AI art](https://johnwesthoff.com/ideas/future_movie/).
